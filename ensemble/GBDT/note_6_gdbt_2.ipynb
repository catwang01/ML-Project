{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[toc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBDT sklearn 源码分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "接下来，我们将研究一下 sklearn 中的 `GradientBoostingRegressor` 的实现\n",
    "\n",
    "```\n",
    "class GradientBoostingRegressor(RegressorMixin, BaseGradientBoosting):\n",
    "```\n",
    "\n",
    "GradientBoostingRegression 继承自 BaseGradientBoosting 类。BaseGradientBoosting 类的源码在 [`scikit-learn/_gb.py`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_gb.py#L133) 中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `_fit_stage`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaseGradientBoosting类有一个 `_fit_stage` 方法，定义了如何进行一次更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,\n",
    "               random_state, X_idx_sorted, X_csc=None, X_csr=None):\n",
    "    \n",
    "    assert sample_mask.dtype == np.bool\n",
    "    loss = self.loss_\n",
    "    original_y = y\n",
    "\n",
    "    raw_predictions_copy = raw_predictions.copy()\n",
    "\n",
    "    for k in range(loss.K):\n",
    "        if loss.is_multi_class:\n",
    "            y = np.array(original_y == k, dtype=np.float64)\n",
    "\n",
    "        residual = loss.negative_gradient(y, raw_predictions_copy, k=k,\n",
    "                                          sample_weight=sample_weight)\n",
    "\n",
    "        # induce regression tree on residuals\n",
    "        tree = DecisionTreeRegressor(\n",
    "            criterion=self.criterion,\n",
    "            splitter='best',\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n",
    "            min_impurity_decrease=self.min_impurity_decrease,\n",
    "            min_impurity_split=self.min_impurity_split,\n",
    "            max_features=self.max_features,\n",
    "            max_leaf_nodes=self.max_leaf_nodes,\n",
    "            random_state=random_state,\n",
    "            ccp_alpha=self.ccp_alpha)\n",
    "\n",
    "        if self.subsample < 1.0:\n",
    "            # no inplace multiplication!\n",
    "            sample_weight = sample_weight * sample_mask.astype(np.float64)\n",
    "\n",
    "        X = X_csr if X_csr is not None else X\n",
    "        tree.fit(X, residual, sample_weight=sample_weight,\n",
    "                 check_input=False, X_idx_sorted=X_idx_sorted)\n",
    "\n",
    "        loss.update_terminal_regions(\n",
    "            tree.tree_, X, y, residual, raw_predictions, sample_weight,\n",
    "            sample_mask, learning_rate=self.learning_rate, k=k)\n",
    "\n",
    "        self.estimators_[i, k] = tree\n",
    "\n",
    "    return raw_predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss.K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，loss 是一个 LossFuncition 对象，代码位于 [`scikit-learn/_gb_losses.py`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_gb_losses.py#L17-L17) 中，部分代码如下：\n",
    "\n",
    "```\n",
    "class LossFunction(metaclass=ABCMeta):\n",
    "    \"\"\"Abstract base class for various loss functions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_classes : int\n",
    "        Number of classes.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    K : int\n",
    "        The number of regression trees to be induced;\n",
    "        1 for regression and binary classification;\n",
    "        ``n_classes`` for multi-class classification.\n",
    "    \"\"\"\n",
    "\n",
    "    is_multi_class = False\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        self.K = n_classes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，.K 是 loss 对象的一个属性，表示有几类。对于回归或二分类问题，K = 1.\n",
    "\n",
    "因此，`_fit_stage` 函数的主体是一个 for 循环，对**每一类**进行循环。而在循环内部，计算了负梯度\n",
    "\n",
    "```\n",
    "residual = loss.negative_gradient(y, raw_predictions_copy, k=k,\n",
    "                                      sample_weight=sample_weight)\n",
    "```\n",
    "\n",
    "然后生成了一个 `DecisionTreeRegressor` 回归树对象，并调用 fit 方法进行学习，其中，target 参数传入的是 residual，也即将负梯度作为目标进行拟合。\n",
    "\n",
    "\n",
    "```\n",
    "tree.fit(X, residual, sample_weight=sample_weight,\n",
    "             check_input=False, X_idx_sorted=X_idx_sorted)\n",
    "```\n",
    "\n",
    "这里可以得到结论：\n",
    "1. 对于回归问题或二分类问题，GBDT 每次迭代拟合一棵树。\n",
    "2. 而对于多分类问题，假设分类数为 K，那么每次会拟合 K 棵树。这些树的输出最后会用来进行 softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss.update_terminal_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 `tree.fit` 调用完之后，有一个 `loss.update_terminal_regions` 的操作，这个函数也是在 LossFunction 类定义的，如下：\n",
    "\n",
    "```\n",
    "    def update_terminal_regions(self, tree, X, y, residual, raw_predictions,\n",
    "                                sample_weight, sample_mask,\n",
    "                                learning_rate=0.1, k=0):\n",
    "        \"\"\"Update the terminal regions (=leaves) of the given tree and\n",
    "        updates the current predictions of the model. Traverses tree\n",
    "        and invokes template method `_update_terminal_region`.\n",
    "        \"\"\"\n",
    "        \n",
    "        # compute leaf for each sample in ``X``.\n",
    "        terminal_regions = tree.apply(X)\n",
    "\n",
    "        # mask all which are not in sample mask.\n",
    "        masked_terminal_regions = terminal_regions.copy()\n",
    "        masked_terminal_regions[~sample_mask] = -1\n",
    "\n",
    "        # update each leaf (= perform line search)\n",
    "        for leaf in np.where(tree.children_left == TREE_LEAF)[0]:\n",
    "            self._update_terminal_region(tree, masked_terminal_regions,\n",
    "                                         leaf, X, y, residual,\n",
    "                                         raw_predictions[:, k], sample_weight)\n",
    "\n",
    "        # update predictions (both in-bag and out-of-bag)\n",
    "        raw_predictions[:, k] += \\\n",
    "            learning_rate * tree.value[:, 0, 0].take(terminal_regions, axis=0)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n",
    "                                residual, raw_predictions, sample_weight):\n",
    "        \"\"\"Template method for updating terminal regions (i.e., leaves).\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "根据函数的描述可以看出，update_terminal_regions 有两个作用，一个是修改参数 tree 的叶节点的输出。一个是更新参数 raw_predictions。更新叶节点的输出时，调用了 `self._update_terminal_region`，而更新 raw_predictions 是直接将预测值乘以 learning_rate 加到 raw_predictions上。\n",
    "\n",
    "而 `_update_terminal_region` 函数是用 abstractmethod 修改的抽象函数，是在子类中定义的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于 `GradientBoostingRegressor` 的 Loss 取的是 `LeastSquaresError`，因此我们再看看 `LeastSquaresError` 中的 `update_terminal_region` 函数是如何定义的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def update_terminal_regions(self, tree, X, y, residual, raw_predictions,\n",
    "                                sample_weight, sample_mask,\n",
    "                                learning_rate=0.1, k=0):\n",
    "    \"\"\"Least squares does not need to update terminal regions.\n",
    "    But it has to update the predictions.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : tree.Tree\n",
    "        The tree object.\n",
    "    X : ndarray of shape (n_samples, n_features)\n",
    "        The data array.\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        The target labels.\n",
    "    residual : ndarray of shape (n_samples,)\n",
    "        The residuals (usually the negative gradient).\n",
    "    raw_predictions : ndarray of shape (n_samples, K)\n",
    "        The raw predictions (i.e. values from the tree leaves) of the\n",
    "        tree ensemble at iteration ``i - 1``.\n",
    "    sample_weight : ndarray of shape (n,)\n",
    "        The weight of each sample.\n",
    "    sample_mask : ndarray of shape (n,)\n",
    "        The sample mask to be used.\n",
    "    learning_rate : float, default=0.1\n",
    "        Learning rate shrinks the contribution of each tree by\n",
    "         ``learning_rate``.\n",
    "    k : int, default=0\n",
    "        The index of the estimator being updated.\n",
    "    \"\"\"\n",
    "    # update predictions\n",
    "    raw_predictions[:, k] += learning_rate * tree.predict(X).ravel()\n",
    "\n",
    "def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n",
    "                            residual, raw_predictions, sample_weight):\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据函数描述可以看到，`LeastSquaresError` 不需要修改叶节点的输出值。也就是说，叶节点的输出值就是 `tree.fit` 得到的输出值，只不过在预测的时候需要乘以 learning_rate 之后再累加到之前的结果中。\n",
    "\n",
    "从源码中可以看出来，`GraidentBoostingRegressor` 的实现中，只有一次优化，也就是基学习器的优化，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `_fit_stages`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "了解了基学习器是如何学习的，再看看基学习器的结果是如何组合的。BaseGradientBoosting 类的 `_fit_stages` 方法描述了多个基学习器是如何学习的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,\n",
    "                    X_val, y_val, sample_weight_val,\n",
    "                    begin_at_stage=0, monitor=None, X_idx_sorted=None):\n",
    "        \"\"\"Iteratively fits the stages.\n",
    "\n",
    "        For each stage it computes the progress (OOB, train score)\n",
    "        and delegates to ``_fit_stage``.\n",
    "        Returns the number of stages fit; might differ from ``n_estimators``\n",
    "        due to early stopping.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        do_oob = self.subsample < 1.0\n",
    "        sample_mask = np.ones((n_samples, ), dtype=np.bool)\n",
    "        n_inbag = max(1, int(self.subsample * n_samples))\n",
    "        loss_ = self.loss_\n",
    "\n",
    "        if self.verbose:\n",
    "            verbose_reporter = VerboseReporter(self.verbose)\n",
    "            verbose_reporter.init(self, begin_at_stage)\n",
    "\n",
    "        X_csc = csc_matrix(X) if issparse(X) else None\n",
    "        X_csr = csr_matrix(X) if issparse(X) else None\n",
    "\n",
    "        if self.n_iter_no_change is not None:\n",
    "            loss_history = np.full(self.n_iter_no_change, np.inf)\n",
    "            # We create a generator to get the predictions for X_val after\n",
    "            # the addition of each successive stage\n",
    "            y_val_pred_iter = self._staged_raw_predict(X_val)\n",
    "\n",
    "        # perform boosting iterations\n",
    "        i = begin_at_stage\n",
    "        for i in range(begin_at_stage, self.n_estimators):\n",
    "\n",
    "            # subsampling\n",
    "            if do_oob:\n",
    "                sample_mask = _random_sample_mask(n_samples, n_inbag,\n",
    "                                                  random_state)\n",
    "                # OOB score before adding this stage\n",
    "                old_oob_score = loss_(y[~sample_mask],\n",
    "                                      raw_predictions[~sample_mask],\n",
    "                                      sample_weight[~sample_mask])\n",
    "\n",
    "            # fit next stage of trees\n",
    "            raw_predictions = self._fit_stage(\n",
    "                i, X, y, raw_predictions, sample_weight, sample_mask,\n",
    "                random_state, X_idx_sorted, X_csc, X_csr)\n",
    "\n",
    "            # track deviance (= loss)\n",
    "            if do_oob:\n",
    "                self.train_score_[i] = loss_(y[sample_mask],\n",
    "                                             raw_predictions[sample_mask],\n",
    "                                             sample_weight[sample_mask])\n",
    "                self.oob_improvement_[i] = (\n",
    "                    old_oob_score - loss_(y[~sample_mask],\n",
    "                                          raw_predictions[~sample_mask],\n",
    "                                          sample_weight[~sample_mask]))\n",
    "            else:\n",
    "                # no need to fancy index w/ no subsampling\n",
    "                self.train_score_[i] = loss_(y, raw_predictions, sample_weight)\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                verbose_reporter.update(i, self)\n",
    "\n",
    "            if monitor is not None:\n",
    "                early_stopping = monitor(i, self, locals())\n",
    "                if early_stopping:\n",
    "                    break\n",
    "\n",
    "            # We also provide an early stopping based on the score from\n",
    "            # validation set (X_val, y_val), if n_iter_no_change is set\n",
    "            if self.n_iter_no_change is not None:\n",
    "                # By calling next(y_val_pred_iter), we get the predictions\n",
    "                # for X_val after the addition of the current stage\n",
    "                validation_loss = loss_(y_val, next(y_val_pred_iter),\n",
    "                                        sample_weight_val)\n",
    "\n",
    "                # Require validation_score to be better (less) than at least\n",
    "                # one of the last n_iter_no_change evaluations\n",
    "                if np.any(validation_loss + self.tol < loss_history):\n",
    "                    loss_history[i % len(loss_history)] = validation_loss\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        return i + 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的代码可以看出，学习的过程的逻辑也是挺直接的。\n",
    "\n",
    "主要的内容是一个 for 循环，从 i 从 begin_at_stage 开始遍历到 self.n_estimator。进入 for 循环内部。先判断是否选择 oob，如果是，就进行抽样，并计算此时的袋外误差。\n",
    "\n",
    "之所有不是从 0 开始而是从 begin_at_stage 开始，是为了支持从某个阶段开始继续训练的功能。\n",
    "\n",
    "然后再调用上面讨论过的 `_fit_stage` 进行一个基学习器的学习。学习完之后记录当前 score，并记录袋外估计的 loss 减少（如果有 oob 的话）。\n",
    "\n",
    "最后判断是否需要 early stopping。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的过程实际上已经比较清晰地说明了如何进行基学习器的学习与组合。不过，上面并没有说明如何初始化预测值。根据我们的理论知识，GBDT 对于回归问题是用样本均值来初始化预测值的。因此我们再看看如何初始化代码中是如何进行的。\n",
    "\n",
    "BaseGradietBoosting 的 fit 函数调用进行了第一个基学习器的初始化并调用了 `_fit_stages` 方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:58:56.670044Z",
     "start_time": "2020-07-19T09:58:56.661474Z"
    }
   },
   "source": [
    "不过这个函数代码有点多， 因此就只调我们关心的如何初始化基学习器的部分来看。\n",
    "\n",
    "\n",
    "```\n",
    "def fit(self, X, y, sample_weight=None, monitor=None):\n",
    "    // 这里省略一些代码 ....\n",
    "    if not self._is_initialized():\n",
    "        # init state\n",
    "        self._init_state()\n",
    "\n",
    "        # fit initial model and initialize raw predictions\n",
    "        if self.init_ == 'zero':\n",
    "            raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),\n",
    "                                       dtype=np.float64)\n",
    "        else:\n",
    "            # XXX clean this once we have a support_sample_weight tag\n",
    "            if sample_weight_is_none:\n",
    "                self.init_.fit(X, y)\n",
    "            else:\n",
    "                msg = (\"The initial estimator {} does not support sample \"\n",
    "                       \"weights.\".format(self.init_.__class__.__name__))\n",
    "                try:\n",
    "                    self.init_.fit(X, y, sample_weight=sample_weight)\n",
    "                except TypeError:  # regular estimator without SW support\n",
    "                    raise ValueError(msg)\n",
    "                except ValueError as e:\n",
    "                    if \"pass parameters to specific steps of \"\\\n",
    "                       \"your pipeline using the \"\\\n",
    "                       \"stepname__parameter\" in str(e):  # pipeline\n",
    "                        raise ValueError(msg) from e\n",
    "                    else:  # regular estimator whose input checking failed\n",
    "                        raise\n",
    "\n",
    "            raw_predictions = \\\n",
    "                self.loss_.get_init_raw_predictions(X, self.init_)\n",
    "\n",
    "        begin_at_stage = 0\n",
    "\n",
    "        # The rng state must be preserved if warm_start is True\n",
    "        self._rng = check_random_state(self.random_state)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T10:01:10.454405Z",
     "start_time": "2020-07-19T10:01:10.448878Z"
    }
   },
   "source": [
    "可以看到，初始化基学习器并进行第一次预测的部分在 \n",
    "\n",
    "```\n",
    "raw_predictions = \\\n",
    "                self.loss_.get_init_raw_predictions(X, self.init_)\n",
    "```\n",
    "\n",
    "调用了 `self.loss_` 对象的 `get_init_raw_predictions` 方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看看 `RegressionLossFunction` 的 `get_init_raw_predictions` 方法的内容是什么。`RegressionLossFunction` 继承自 `LossFunction` 类，并且是 `LeastSquaresError` 的父类。\n",
    "\n",
    "```\n",
    "def get_init_raw_predictions(self, X, estimator):\n",
    "    predictions = estimator.predict(X)\n",
    "    return predictions.reshape(-1, 1).astype(np.float64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，这个函数接受一个 X 和一个 estimator，并调用了 estimator 的 predict 对象。而在 `fit` 函数中， estimator 参数传入的是 `self.init_`。\n",
    "\n",
    "而 `self.init_` 又是怎么来的呢？ `self.init_` 是在 `self._init_state()` 的时候进行初始化的。我们可以看到，fit 函数中有这样的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "if not self._is_initialized():\n",
    "    # init state\n",
    "    self._init_state()\n",
    "```\n",
    "\n",
    "说明如果没有初始化，那么就调用 `self._init_state()` 进行初始化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们再进入 `BaseGradientBooting` 的 `_init_state` 方法中看一看它是如何初始化 `self._init` 的。\n",
    " \n",
    "```\n",
    "def _init_state(self):\n",
    "    \"\"\"Initialize model state and allocate model state data structures. \"\"\"\n",
    "\n",
    "    self.init_ = self.init\n",
    "    if self.init_ is None:\n",
    "        self.init_ = self.loss_.init_estimator()\n",
    "\n",
    "    self.estimators_ = np.empty((self.n_estimators, self.loss_.K),\n",
    "                                dtype=np.object)\n",
    "    self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)\n",
    "    # do oob?\n",
    "    if self.subsample < 1.0:\n",
    "        self.oob_improvement_ = np.zeros((self.n_estimators),\n",
    "                                         dtype=np.float64)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们看看 `init_estimator` 是如何定义的。`init_estimator` 定义在 `LeastSquaresError` 中，我们看看\n",
    "\n",
    "```\n",
    "def init_estimator(self):\n",
    "    return DummyRegressor(strategy='mean')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('tensorflow2': conda)",
   "language": "python",
   "name": "python361064bittensorflow2conda916f6dc8789a43e39b82205c8a731f83"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "193.1875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
