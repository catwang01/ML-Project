{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[toc]\n",
    "\n",
    "# XGBoost 调参指南"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T04:03:07.675097Z",
     "start_time": "2020-07-15T04:03:07.664135Z"
    }
   },
   "source": [
    "关于 Python 的 XGBoost 的基本使用可以看：[Python Package Introduction — xgboost 1.2.0-SNAPSHOT documentation](https://xgboost.readthedocs.io/en/latest/python/python_intro.html)\n",
    "\n",
    "XGBoost 在 Python 中的实现有两种，一种是和 sklearn 结合的实现，感觉这种实现支持的特性会少一些。因此个人不使用 sklearn 的 api。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:32:02.030007Z",
     "start_time": "2020-07-15T10:32:02.019943Z"
    }
   },
   "source": [
    "首先，XGBoost 有三类参数可以调节。\n",
    "\n",
    "Before running XGBoost, we must set three types of parameters: \n",
    "- **General parameters** relate to which booster we are using to do boosting, commonly tree or linear model\n",
    "\n",
    "- **Booster parameters** depend on which booster you have chosen\n",
    "\n",
    "- **Learning task parameters** decide on the learning scenario. For example, regression tasks may use different parameters with ranking tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:32:56.452416Z",
     "start_time": "2020-07-15T10:32:56.448847Z"
    }
   },
   "source": [
    "### General parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. objective: \n",
    "    回归用 reg:squarederror, 分类用 binary:logistic 或 multi:softmax。具体可以看 [XGBoost Parameters — xgboost 1.2.0-SNAPSHOT documentation](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters)\n",
    "2. eta: XGBoost的learning rate，$f_t = f_{t-1} + \\eta h_t(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Booster Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Booster Parameters 和具体所选择的 Booster 有关。这里以 gbtree 为例。gbtree 通常为 reglinear 的效果要好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T11:01:25.516970Z",
     "start_time": "2020-07-15T11:01:25.508088Z"
    }
   },
   "source": [
    "1. eta [default=0.3]\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "\n",
    "\n",
    "3. max_depth [default=6]\n",
    "    - The maximum depth of a tree, Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10"
   ]
  },
  {
   "source": [
    "## 调参思路\n",
    "\n",
    "大的思路是 learning rate 从大到小，然后逐步微调其他参数。\n",
    "\n",
    "1. 由于树的个数可以用 early-stop 确定，因此不需要调这个参数。因此我们先控制 learning rate 比较大（0.1 左右），此时可以大致确定其他树参数的范围。（为什么一开始控制 learning-rate 比较大？因为大 learning-rate 收敛比较快。我们这个阶段的目的是粗估其他参数的范围，因此不需要很精细）\n",
    "2. 然后我们在逐步缩小 learning rate, 微调其他的树参数的值。\n",
    "\n",
    "在调整其他树的参数的值时候，也有一定的顺序。\n",
    "\n",
    "1. 一般来说，树的深度对模型的影响比较大，因此优先去调。\n",
    "2. 然后去调树的最大节点数、每个节点的最小样本数等等参数。\n",
    "3. 正则化参数一般放在后面再调。因为我们是在模型效果比较好，出现过拟合现象时采取使用正则化技术缓解过拟合，来获取在验证集上的提升。因此不需要最开始就调这些参数。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python 实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以参考 [ 2 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:36:04.256489Z",
     "start_time": "2020-07-15T10:36:04.252203Z"
    }
   },
   "source": [
    "### xgb.DMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T05:22:10.909740Z",
     "start_time": "2020-08-19T05:22:10.831551Z"
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgb'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7d2712b9c009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgb'"
     ]
    }
   ],
   "source": [
    "import xgb\n",
    "dval = xgb.DMatrix(x_val, y_val)\n",
    "dtest = xgb.DMatrix(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:33:55.898619Z",
     "start_time": "2020-07-15T10:33:55.895225Z"
    }
   },
   "source": [
    "### EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "train(..., evals=evals, early_stopping_rounds=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T04:02:59.764245Z",
     "start_time": "2020-07-15T04:02:59.672509Z"
    }
   },
   "source": [
    "# References\n",
    "1. [XGBoost Parameters — xgboost 1.2.0-SNAPSHOT documentation](https://xgboost.readthedocs.io/en/latest/parameter.html#xgboost-parameters)\n",
    "2. [Python Package Introduction — xgboost 1.2.0-SNAPSHOT documentation](https://xgboost.readthedocs.io/en/latest/python/python_intro.html#early-stopping)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('tensorflow2': conda)",
   "language": "python",
   "name": "python361064bittensorflow2conda916f6dc8789a43e39b82205c8a731f83"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}