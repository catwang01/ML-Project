{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Logistic Regression\n",
    "tags: 小书匠,lr,cross_entropy,job,Logistic,LR,note|笔记,notes,sigmoid,softmax\n",
    "grammar_cjkRuby: true\n",
    "renderNumberedHeading: true\n",
    "---\n",
    "\n",
    "[toc]\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "## 1. LR的损失函数公式\n",
    "\n",
    "多分类问题的损失函数\n",
    "\n",
    "从单样本二分类慢慢过渡。\n",
    "\n",
    "### 单样本二分类\n",
    "\n",
    "$$\n",
    "L(y_i, \\hat{y}_i) = - y_i \\log \\frac{1}{1 + \\exp x_i^T \\beta } - (1-y_i) \\log \\frac{\\exp x_i^T \\beta }{1 + \\exp x_i^T \\beta }\n",
    "$$ \n",
    "\n",
    "### 单样本多分类\n",
    "\n",
    "$$\n",
    "L(y_i, \\hat{y}_i) = - y_{i0} \\log \\frac{1}{1 + \\sum_{i=1}^{K-1} \\exp x_i^T \\beta_i } - y_{i1} \\log \\frac{\\exp x_i^T \\beta_1 }{1 + \\sum_{i=1}^{K-1} \\exp x_i^T \\beta_1 }  \\ldots - y_{i,K-1} \\log \\frac{\\exp x_i^T \\beta_{K-1} }{1 + \\sum_{i=1}^{K-1} \\exp x_i^T \\beta_{K-1} } \n",
    "$$ \n",
    "\n",
    "### 多样本多分类\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = \\sum_{i=1}^{N}  \\left(- y_{i0} \\log \\frac{1}{1 + \\sum_{i=1}^{K-1} \\exp x_i^T \\beta_i }  - y_{i1} \\log \\frac{\\exp x_i^T \\beta_1 }{1 + \\sum_{i=1}^{K-1} \\exp x_i^T \\beta_1 }  \\ldots - y_{i,K-1} \\log \\frac{\\exp x_i^T \\beta_{K-1} }{1 + \\sum_{i=1}^{K-1} \\exp x_i^T \\beta_{K-1} } \\right)\n",
    "$$ \n",
    "\n",
    "## 2. LR 的 损失函数公式的推导\n",
    "\n",
    "### 统计视角\n",
    "\n",
    "#### 根据极大似然估计可以得到\n",
    "\n",
    "假设 $y \\sim Bernoulli(\\pi(x))$ ，\n",
    "\n",
    "其中 $\\pi(x)$ 是关于 $x$  的线性函数。参数为 $\\beta$ 即\n",
    "\n",
    "$$\n",
    "\\pi(x;\\beta ) = x^T \\beta\n",
    "$$ \n",
    "\n",
    "则样本 $i$ 的似然函数为\n",
    "\n",
    "$$\n",
    "L(x_i;\\beta) = \\pi_i^{y_i} (1-\\pi_i)^{1-y_i}\n",
    "$$ \n",
    "\n",
    "取对数，仍然记作 $L$ \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    L(x_i;\\beta) &= y_i \\log \\pi_i + (1-y_i) \\log (1-\\pi_i) \\\\\n",
    "    &= y_i \\log x_i^T \\beta + (1-y_i) \\log (1-x_i^T \\beta ) \\\\\n",
    "\\end{aligned}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 统计视角2\n",
    "\n",
    "上面的是一种比较统计的视角，其中引入了 bernoulli 分布（如果是多分类就是 multinominal 分布）。\n",
    "\n",
    "还有一种常见的思路，这种思路没有显式提到概率分布，他们就直接给出一个式子\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(x_i) = \\frac{sofmax(f(x_i | \\theta))}{\\sum_{j=1}^n sofmtax(f(x_j|\\theta)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息论角度：\n",
    "\n",
    "真实分布: $y \\sim Bernoulli(\\pi(x))$ ，\n",
    "\n",
    "关于样本分布，理解起来可能有点费劲：\n",
    "1. 若样本 $y_i = 1$ ，则表明 $P(y_i=1)=1, P(y_i=0)=0$\n",
    "2. 若样本 $y_i = 0$ ，则表明 $P(y_i=1)=0, P(y_i=1)=1$\n",
    "\n",
    "即 $P(y_i=1)=y_i, P(y_i=0)=1-y_i$ \n",
    "\n",
    "因此计算真实分布和样本分布的交叉熵\n",
    "\n",
    "$$\n",
    "L(y_i, \\hat{y}_i) = y_i \\log \\pi_i  + (1-y_i) \\log (1 - \\pi_i)\n",
    "$$ \n",
    "\n",
    "\n",
    "（来自李宏毅ppt，注意$\\hat{y}$表示的是样本值，和一般的表示方法不同）\n",
    "![picture from 李宏毅](https://gitee.com/EdwardElric_1683260718/picture_bed/raw/master/img/20200428202534.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T06:20:15.714108Z",
     "start_time": "2020-08-27T06:20:15.706812Z"
    }
   },
   "source": [
    "## 2. 为什么LR不使用MSE做损失函数 / 为什么使用交叉熵做损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T06:26:04.927524Z",
     "start_time": "2020-08-27T06:26:04.922446Z"
    }
   },
   "source": [
    "### 似然函数的角度\n",
    "\n",
    "假设有 n 个样本 $\\{x_{i}, y_i\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息论的角度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "H(p, q) = \\sum_{x} p(x) log \\frac{1}{q(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL 散度 = 交叉熵 - 熵，优化交叉熵相当于优化两个分布之间的距离。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T06:20:58.308624Z",
     "start_time": "2020-08-27T06:20:58.299513Z"
    }
   },
   "source": [
    "### 梯度下降的角度\n",
    "\n",
    "因为当前值远离最优解时，梯度小，不容易收敛。\n",
    "\n",
    "预测值为\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + \\exp (-x^T \\beta )}\n",
    "$$ \n",
    "\n",
    "如果取 MSE 为损失函数，有\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^{N} (y_i - f(x_i))^2\n",
    "$$ \n",
    "\n",
    "对 $\\beta$  求导\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial  \\beta }  = -2 \\sum_{i=1}^{N} (y - f(x_i)) f(x_i) (1 - f(x_i)) x_i\n",
    "\\end{aligned}\n",
    "$$ \n",
    "\n",
    "假设 $y_i=1$ \n",
    "1. 若 $f(x_i)=1$ 时，上式为 0，因此梯度不会更新。这是合理的，因为已经到达了最优值。\n",
    "2. 若 $f(x_i)=0$ 时，上式为 0，因此梯度不会更新。这是不合理的，因为明明估计是很差的，但是梯度却为0，因此参数无法更新，所以模型难以优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 为什么用 logistic 函数作为link function？\n",
    "\n",
    "logistic函数：\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1+ e^{-x}}\n",
    "$$\n",
    "\n",
    "1. 定义域为 $(- \\infty, + \\infty)$，值域为 $(0, 1)$，因此可以压缩将任意值压缩到$(0, 1)$，形成一个概率。\n",
    "2. 光滑，容易求导。\n",
    "\n",
    "导数满足 $f'(x) = f(x)(1-f(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LR的推导过程\n",
    "\n",
    "Motivation: 如何用线性回归来解决分类问题？\n",
    "\n",
    "1. 不是预测分类本身，而是预测概率。\n",
    "\n",
    "线性回归不能保证预测出来的是概率，因此我们对线性回归的结果$y=X\\beta$上面再套一个非线性函数将其压缩到$(0,1)$，因此会得到一个概率。\n",
    "\n",
    "这个非线性函数是 sigmoid 函数，因此得到:\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1 + e ^{-X\\beta}}\n",
    "$$\n",
    "\n",
    "![\\picture goes here](https://gitee.com/EdwardElric_1683260718/picture_bed/raw/master/img/20200428210052.png)\n",
    "\n",
    "\n",
    "## 5. LR如何解决共线性，为什么深度学习不强调\n",
    "\n",
    "加二阶正则项，加二阶正则项相当于进行了 guassian 分布的先验。\n",
    "\n",
    "共线性是源于线性模型，而深度学习并不是线性模型。所以说深度学习并不需要进行解决共线性问题。\n",
    "\n",
    "## 6. LR如何防止过拟合\n",
    "\n",
    "加正则项\n",
    "\n",
    "## 7. LR分布式训练怎么做"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LR 的随机梯度实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以多样本二分类为例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T11:44:12.487190Z",
     "start_time": "2020-07-18T11:44:12.483122Z"
    }
   },
   "source": [
    "$$\n",
    "\\hat{y} = \\frac{1}{ 1 + exp(-x^T \\beta + b)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T12:20:48.786730Z",
     "start_time": "2020-07-18T12:20:48.764387Z"
    }
   },
   "source": [
    "$$\n",
    "L = \\sum_{i=1}^N l( \\hat{y_i}, y_i)\n",
    "$$ \n",
    "\n",
    "其中 \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "l( \\hat{y}, y) &= - \\left(y_i \\log \\hat{y_i} + (1 - y_i) log (1 - \\hat{y_i}) \\right) \\\\\n",
    "&= -\\left( y_i \\log \\frac{1}{1  + \\exp (-\\left( x_i^T \\beta + b \\right) )} + (1- y_i) \\log \\frac{ \\exp(-\\left( x_i^T \\beta + b \\right) )}{1 + \\exp(-\\left( x_i^T \\beta + b \\right) )}\\right)  \\\\\n",
    "&=  - \\left(y_i \\log \\frac{1}{1 + \\exp(-\\left( x_i^T \\beta + b \\right) )} + \\left( 1 - y_i \\right) \\log \\exp\\left( -\\left( x_i^T \\beta +b  \\right) \\right) + \\left( 1 - y_i \\right) \\log \\frac{1}{1 + \\exp\\left( -\\left( x_i^T \\beta + b  \\right) \\right) } \\right) \\\\\n",
    "&=  - \\left( \\log \\frac{1}{1 + exp(-\\left( x_i^T \\beta + b \\right) )} + \\left( 1 - y_i \\right) \\left( -\\left( x_i^T \\beta + b \\right)  \\right)  \\right) \\\\\n",
    "&= -\\left( \\left( 1 - y_i\\right) \\left( -\\left( x_i^T \\beta + b  \\right) \\right) - \\log (1 + \\exp \\left(-\\left( x ^T \\beta + b  \\right) \\right)  \\right)  \\\\\n",
    "&=  \\log \\left(1 + \\exp \\left(-\\left( x ^T \\beta + b  \\right) \\right)  \\right) + \\left( 1 - y_i\\right) \\left( x_i^T \\beta + b  \\right)\n",
    " .\\end{aligned}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l\\left( \\hat{y_i}, y_i \\right) }{\\partial \\beta}  &= -\\left( - \\left( 1 - y_i \\right) x_i  + \\frac{1}{1  + \\exp (-x_i^T \\beta + b)} \\exp\\left( -x_i^T \\beta + b \\right) x_i \\right)\\\\\n",
    "&= - \\left( -\\left( 1 - y_i \\right) x_i + \\left( 1 - \\hat{y_i}\\right) x_i  \\right)  \\\\\n",
    "&= - \\left( y_i - \\hat{y_i} \\right)x_i  \\\\\n",
    "&= \\left( \\hat{y_i} - y_i \\right) x_i  \\\\\n",
    ".\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial l\\left( \\hat{y_i}, y_i \\right) }{\\partial b}  &= - \\left( - \\left( 1 - y_i \\right) + \\frac{\\exp\\left( -x ^T \\beta + b \\right) }{1 + \\exp \\left( -x ^T \\beta + b \\right) } \\right)  \\\\\n",
    "    &= - \\left( - \\left( 1 - y_i \\right) + \\left( 1 - \\hat{y}_i \\right)  \\right)  \\\\\n",
    "    &= - \\left( y_i - \\hat{y}_i \\right)  \\\\\n",
    "    &= \\hat{y}_i - y_i \\\\\n",
    ".\\end{aligned}\n",
    "$$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T12:44:13.571347Z",
     "start_time": "2020-07-18T12:44:13.568582Z"
    }
   },
   "source": [
    "## 10. 什么是线性模型？ LR 线性模型吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考 [ 2 ]\n",
    "\n",
    "线性模型的线性是指参数是以线性组合的方式结合起来的。对于分类问题来说，线性模型指的是决策边界是线性的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策边界是指分类为使 p(y=1|x) = p(y=0|x) 相等的那条线。对于 LR 模型来说，决策边界的推导如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T12:47:56.700906Z",
     "start_time": "2020-07-18T12:47:56.696000Z"
    }
   },
   "source": [
    "$$\n",
    "P(y=1|x)= \\frac{1}{1 + \\exp(-(x^T \\beta + b))}\n",
    "$$\n",
    "\n",
    "因此"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "P(y=1|x) &= P(y=0|x) \\\\\n",
    "\\frac{1}{1 + \\exp(-(x^T \\beta + b))} &= \\frac{exp(-(x^T \\beta + b))}{1 + \\exp(-(x^T \\beta + b))} \\\\\n",
    "1 &= \\exp(-(x^T \\beta + b)) \\\\\n",
    "0 &= -(x^T \\beta + b) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "即决策边界为 \n",
    "$$x^T \\beta + b = 0$$\n",
    "这是线性的，因此，LR 是线性模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Tensorflow 写出 LR 的训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T13:17:04.415638Z",
     "start_time": "2020-07-18T13:17:03.890062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x160c05470>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "\n",
    "iris = load_iris()\n",
    "x = iris['data'].astype(np.float32)\n",
    "y = iris['target'].astype(np.int32)\n",
    "\n",
    "def get_weight(shape):\n",
    "    return tf.Variable(tf.random.normal(shape), dtype=tf.float32)\n",
    "\n",
    "def get_bias(shape):\n",
    "    return tf.Variable(tf.zeros(shape), dtype=tf.float32)\n",
    "\n",
    "input_size = 4\n",
    "output_size = 3\n",
    "\n",
    "w = get_weight((input_size, output_size))\n",
    "b = get_bias((1, output_size))\n",
    "\n",
    "def forward(x, w, b):\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "n_epochs = 100\n",
    "optimizer = tf.keras.optimizers.SGD(0.1)\n",
    "loss_val = []\n",
    "for epoch in range(n_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = forward(x, w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    grads = tape.gradient(loss, [w, b])\n",
    "    optimizer.apply_gradients(zip(grads, [w, b]))\n",
    "    loss_val.append(loss.numpy())\n",
    "plt.plot(loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. lr 适合什么样的数据 / （为什么 lr 适合处理离散数据）？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要明确一点。工业界一般不会直接使用连续特征来训练 lr，一般都会离散化之后来训练 lr。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 稀疏向量的乘法计算更容易计算。实际上都不需要进行 one-hot 操作，直接使用 labelencoder + embedding_lookup 就可以实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 可以增强拟合能力。做完 one-hot 之后，每个类别都会有一个权重，可以**引入非线性**大大增强模型的表达能力。否则，多个类别公用一个权重。\n",
    "\n",
    "关于引入非线性这一点，可以举一个例子，假设有个变量是年龄。如果不分桶进行onehot，而直接用年龄作为变量，那么 20 岁到30 岁之间的差距和 70 岁到 80岁之间的差距是相同的。如果在风险评估场合，一个人 20 岁 和 30 岁的信用评级可能相差很大（因为 20 岁时还没有什么收入，而 30岁时一般会有比较稳定的收入），而 70 岁到80 岁之间到差距比较小（因为70岁和80岁的风险都比较大）。\n",
    "\n",
    "而如果分桶，将 20 分为一类，30分为一类，70分为一类，80分为一类，那么这三类会分别学出不同的权重，因此模型的表达能力会增强。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 对异常数据更加稳健\n",
    "\n",
    "如果20 - 30 岁分一个桶的话，那么会允许一个数据在 20-29 之间波动而不会影响结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T06:26:31.463454Z",
     "start_time": "2020-08-27T06:26:31.459009Z"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "\n",
    "1. [LR公式的推导 - dpwang - CSDN博客](https://blog.csdn.net/dpengwang/article/details/86746233)\n",
    "2. [为什么说逻辑回归LR是线性分类器？_成长之路-CSDN博客_lr明明是分类模型为什么叫回归](https://blog.csdn.net/xfwdxt/article/details/102516650)\n",
    "3. [(3条消息)交叉熵损失函数原理详解_Cigar-CSDN博客](https://blog.csdn.net/b1055077005/article/details/100152102)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('tensorflow2': conda)",
   "language": "python",
   "name": "python361064bittensorflow2conda916f6dc8789a43e39b82205c8a731f83"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
