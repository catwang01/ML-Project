[toc]


# Job 决策树 面试题整理


## 1. 什么是决策树？

从三个角度来理解。

1. 一棵树
2. if-else规则的集合。从根结点到叶结点的路径实际上决定了一个集合
3. 定义在特征空间与类空间上的条件概率分布，决策树实际上是将特征空间划分成了互不相交的单元，每个从根到叶的路径对应着一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。实际中，哪个类别有较高的条件概率，就把该单元中的实例强行划分为该类别。

因此，对于一棵分类决策树可以表示为

$$y = \sum_{D_i \subset D} c_i I(x \in D_i)$$

其中， $\{D_i\}$ 是集合 D 的一个分割。$D_i$ 就是由根结点到叶结点上的 if-else 规则决定的集合。$c_i$ 是在这个集合上的取值。


## 2. 决策树与其它模型相比的优点

1. 可解释性
2. 计算速度快

## 3. 如何学习一棵决策树

终止的条件： 1. 所有样本都被正确分类 2. 没有可用于分类的特征。

## 4. 

## 5. 选择特征时的指标

有三个
1. 信息增益，对于 ID3
2. 信息增益比，对应 C4.5
3. Gini系数，对就CART分类树
4. 方差 对应 CART 回归树

这些指标虽然比较多。但是定义的模式都差不多。有下面几步

1. 定义对于某个概率分布的指标
2. 定义对于某个条件分布的指标。
3. 定义对于某个集合的指标。是什么。这个相当于将样本经验分布代入了1中的公式
4. 定义了对于在固定某个特征时，对于某个集合的指标是什么。这个样当于将样本经验分布代入了2中的公式

### 1. 信息增益


这个实际上就是熵的减少值。

#### 熵

首先定义什么是熵。

- 对于一个随机变量X，服从离散分布 $X \sim p_i, i=1,\dots, n$，其熵定义为
$$H(p) = - \sum_{i=1}^n p_i \log p_i$$
注意前面有一个负号。因为熵是正的。而 $\log p_i$ 为负，因此需要在前面加一个负号来保证熵是正的。

#### 条件熵

其次定义什么是条件熵。很简单，就是条件分布的熵做一个加权平均。

- 对于假设有两个随机变量 X, Y，X 仍然服从离散分布 $X \sim p_i, i=1,\dots, n$。

固定 $X_i$，$Y | X = x_i$ 是一个关于 Y 的分布，因此可以计算其熵 $H(Y|X=x_i)$。则 Y 对 X 的条件熵定义为

$$ 
\begin{aligned}
H(Y|X) &= E_{X\sim p_i} H(Y|X)\\
&= \sum_{i=1}^n p_i H(Y|X=x_i) 
\end{aligned}
$$

上面的熵是对于一个分布（或者说是服从这个分布的随机变量）所定义的。下面我们将这个概念扩展到一个集合上。

假设一个集合 $D = \{y_i\}_{i=1}^n$，其中有 n 个元素，这些元素可以被分成 K 类，即$D_k, k=1, \dots, K$ 是 D 的一个分割。则 D 的熵定义为

$$H(D) = - \sum_{i=1}^K \frac{|D_k|}{|D|} \log \frac{|D_k|}{|D|}$$

可以看到，这个定义实际上只是将在 D 上的样本分布代入熵的定义中即可。

同时，对于一个特征 A 来说，假设A有m个取值，A 相当于一个随机变量，那么在A的条件下D的熵自然会被定义为，根据 A 的取值会将 D 划分 为 m 个不相交集合 $D^1, \dots, D^m$，则对于 $D^i$ 来说，可以按照上面的定义计算其熵


这边好麻烦呀，不写了。

### 2. 信息增益比

## 为什么用信息增益比？

因为信息增益对于可取值数目较多的属性有所偏好。

# References

1. [数据挖掘面试题之决策树必知必会 - 简书](https://www.jianshu.com/p/fb97b21aeb1d)
